{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packeages\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "from pandas.io.json import json_normalize\n",
    "from typing import List, Dict\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/meena/dev/pyPrediktorMapClient/src/pyprediktormapclient\n"
     ]
    }
   ],
   "source": [
    "# Setting the path\n",
    "module_path = os.path.abspath(os.path.join(\"../src/pyprediktormapclient/\"))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model index functions\n",
    "from model_index import ModelIndex\n",
    "\n",
    "# Import OPC UA functions\n",
    "from opc_ua import OPC_UA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to the servers\n",
    "model_index_url = \"http://10.241.68.86:7001/v1/\"\n",
    "opc_url = \"http://10.241.68.86:13371/\"\n",
    "\n",
    "# Model index API\n",
    "mdx = ModelIndex(url=model_index_url)\n",
    "\n",
    "# OPC UA API\n",
    "opc = OPC_UA(url=opc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings set type data of the site BR-AP\n",
    "strings = mdx.get_object_descendants(\"StringSetType\", ['3:1:SSO.BR-AP'], \"PV_Assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object ids of strings set type objects\n",
    "string_objects_ids = [x['DescendantId'] for x in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the json data into dataframe\n",
    "#strings_df = mdx.expand_props_vars(strings)\n",
    "#strings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strings_df1 = strings_df[:5]\n",
    "#strings_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node ids of the variables (Nodes)\n",
    "vars_node_ids = mdx.get_vars_node_ids(strings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars_node_ids1 = vars_node_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters for value data\n",
    "server_url = \"opc.tcp://10.241.80.4:4872\"\n",
    "# Parameters for aggregate historical data\n",
    "start_time = \"2022-07-05T07:55:14.544000Z\" # 5th July aggregated data \n",
    "end_time = \"2022-07-06T07:55:14.544000Z\"\n",
    "pro_interval = 600000 # 10 minutes processing time\n",
    "agg_name = \"Average\"\n",
    "\n",
    "pro_interval1 = 3600000 # 1 hour processing time\n",
    "end_time1 = \"2022-07-9T07:55:14.544000Z\" # 5 days\n",
    "end_time2 = \"2022-07-14T07:55:14.544000Z\" # 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0_0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "large_agg_hist_10D = opc.get_agg_hist_value_chunks(server_url, start_time, end_time2, pro_interval, agg_name, vars_node_ids, 100000, 5000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0_0\n",
      "Success: 0_1\n",
      "Success: 0_2\n",
      "Success: 0_3\n",
      "Success: 0_4\n",
      "Success: 0_5\n",
      "Success: 0_6\n",
      "Success: 0_7\n",
      "Success: 0_8\n",
      "Success: 0_9\n",
      "Success: 0_10\n",
      "Success: 0_11\n",
      "Success: 0_12\n",
      "Success: 0_13\n",
      "Success: 0_14\n",
      "Success: 0_15\n",
      "Success: 1_0\n",
      "Success: 1_1\n",
      "Success: 1_2\n",
      "Success: 1_3\n",
      "Success: 1_4\n",
      "Success: 1_5\n",
      "Success: 1_6\n",
      "Success: 1_7\n",
      "Success: 1_8\n",
      "Success: 1_9\n",
      "Success: 1_10\n",
      "Success: 1_11\n",
      "Success: 1_12\n",
      "Success: 1_13\n",
      "Success: 1_14\n",
      "Success: 1_15\n",
      "Success: 2_0\n",
      "Success: 2_1\n",
      "Success: 2_2\n",
      "Success: 2_3\n",
      "Success: 2_4\n",
      "Success: 2_5\n",
      "Success: 2_6\n",
      "Success: 2_7\n",
      "Success: 2_8\n",
      "Success: 2_9\n",
      "Success: 2_10\n",
      "Success: 2_11\n",
      "Success: 2_12\n",
      "Success: 2_13\n",
      "Success: 2_14\n",
      "Success: 2_15\n",
      "Success: 3_0\n",
      "Success: 3_1\n",
      "Success: 3_2\n",
      "Success: 3_3\n",
      "Success: 3_4\n",
      "Success: 3_5\n",
      "Success: 3_6\n",
      "Success: 3_7\n",
      "Success: 3_8\n",
      "Success: 3_9\n",
      "Success: 3_10\n",
      "Success: 3_11\n",
      "Success: 3_12\n",
      "Success: 3_13\n",
      "Success: 3_14\n",
      "Success: 3_15\n",
      "Success: 4_0\n",
      "Success: 4_1\n",
      "Success: 4_2\n",
      "Success: 4_3\n",
      "Success: 4_4\n",
      "Success: 4_5\n",
      "Success: 4_6\n",
      "Success: 4_7\n",
      "Success: 4_8\n",
      "Success: 4_9\n",
      "Success: 4_10\n",
      "Success: 4_11\n",
      "Success: 4_12\n",
      "Success: 4_13\n",
      "Success: 4_14\n",
      "Success: 4_15\n",
      "Success: 5_0\n",
      "Success: 5_1\n",
      "Success: 5_2\n",
      "Success: 5_3\n",
      "Success: 5_4\n",
      "Success: 5_5\n",
      "Success: 5_6\n",
      "Success: 5_7\n",
      "Success: 5_8\n",
      "Success: 5_9\n",
      "Success: 5_10\n",
      "Success: 5_11\n",
      "Success: 5_12\n",
      "Success: 5_13\n",
      "Success: 5_14\n",
      "Success: 5_15\n",
      "Success: 6_0\n",
      "Success: 6_1\n",
      "Success: 6_2\n",
      "Success: 6_3\n",
      "Success: 6_4\n",
      "Success: 6_5\n",
      "Success: 6_6\n",
      "Success: 6_7\n",
      "Success: 6_8\n",
      "Success: 6_9\n",
      "Success: 6_10\n",
      "Success: 6_11\n",
      "Success: 6_12\n",
      "Success: 6_13\n",
      "Success: 6_14\n",
      "Success: 6_15\n",
      "Success: 7_0\n",
      "Success: 7_1\n",
      "Success: 7_2\n",
      "Success: 7_3\n",
      "Success: 7_4\n",
      "Success: 7_5\n",
      "Success: 7_6\n",
      "Success: 7_7\n",
      "Success: 7_8\n",
      "Success: 7_9\n",
      "Success: 7_10\n",
      "Success: 7_11\n",
      "Success: 7_12\n",
      "Success: 7_13\n",
      "Success: 7_14\n",
      "Success: 7_15\n",
      "Success: 8_0\n",
      "Success: 8_1\n",
      "Success: 8_2\n",
      "Success: 8_3\n",
      "Success: 8_4\n",
      "Success: 8_5\n",
      "Success: 8_6\n",
      "Success: 8_7\n",
      "Success: 8_8\n",
      "Success: 8_9\n",
      "Success: 8_10\n",
      "Success: 8_11\n",
      "Success: 8_12\n",
      "Success: 8_13\n",
      "Success: 8_14\n",
      "Success: 8_15\n",
      "Success: 9_0\n",
      "Success: 9_1\n"
     ]
    }
   ],
   "source": [
    "large_agg_hist2 = opc.get_agg_hist_value_chunks(server_url, start_time, end_time, pro_interval, agg_name, vars_node_ids, 100000, 1000, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 145000 strings set node ids with 4 max workers and ids bach  size 1000 took about 24 minutes and 17 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0_0\n",
      "Success: 0_1\n",
      "Success: 0_2\n",
      "Success: 0_3\n",
      "Success: 0_4\n",
      "Success: 0_5\n",
      "Success: 0_6\n",
      "Success: 0_7\n",
      "Success: 0_8\n",
      "Success: 0_9\n",
      "Success: 0_10\n",
      "Success: 0_11\n",
      "Success: 0_12\n",
      "Success: 0_13\n",
      "Success: 0_14\n",
      "Success: 0_15\n",
      "Success: 0_16\n",
      "Success: 0_17\n",
      "Success: 0_18\n",
      "Success: 0_19\n",
      "Success: 1_0\n",
      "Success: 1_1\n",
      "Success: 1_2\n",
      "Success: 1_3\n",
      "Success: 1_4\n",
      "Success: 1_5\n",
      "Success: 1_6\n",
      "Success: 1_7\n",
      "Success: 1_8\n",
      "Success: 1_9\n"
     ]
    }
   ],
   "source": [
    "large_agg_hist5 = opc.get_agg_hist_value_chunks(server_url, start_time, end_time, pro_interval, agg_name, vars_node_ids, 100000, 5000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0_0\n",
      "Success: 0_1\n",
      "Success: 0_2\n",
      "Success: 0_3\n",
      "Success: 0_4\n",
      "Success: 0_5\n",
      "Success: 0_6\n",
      "Success: 0_7\n",
      "Success: 0_8\n",
      "Success: 0_9\n",
      "Success: 0_10\n",
      "Success: 0_11\n",
      "Success: 0_12\n",
      "Success: 0_13\n",
      "Success: 0_14\n",
      "Success: 0_15\n",
      "Success: 1_0\n",
      "Success: 1_1\n",
      "Success: 1_2\n",
      "Success: 1_3\n",
      "Success: 1_4\n",
      "Success: 1_5\n",
      "Success: 1_6\n",
      "Success: 1_7\n",
      "Success: 1_8\n",
      "Success: 1_9\n",
      "Success: 1_10\n",
      "Success: 1_11\n",
      "Success: 1_12\n",
      "Success: 1_13\n"
     ]
    }
   ],
   "source": [
    "large_agg_hist = opc.get_agg_hist_value_chunks(server_url, start_time, end_time, pro_interval, agg_name, vars_node_ids, 100000, 5000, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 145000 strings set node ids with 4 max workers and ids bach  size 5000 took about 24 minutes and 57 seconds. Proccessing interval 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0\n",
      "Success: 1\n",
      "Success: 2\n",
      "Success: 3\n",
      "Success: 4\n",
      "Success: 5\n",
      "Success: 6\n",
      "Success: 7\n",
      "Success: 8\n",
      "Success: 9\n",
      "Success: 10\n",
      "Success: 11\n",
      "Success: 12\n",
      "Success: 13\n",
      "Success: 14\n",
      "Success: 15\n",
      "Success: 16\n",
      "Success: 17\n",
      "Success: 18\n",
      "Success: 19\n",
      "Success: 20\n",
      "Success: 21\n",
      "Success: 22\n",
      "Success: 23\n",
      "Success: 24\n",
      "Success: 25\n",
      "Success: 26\n",
      "Success: 27\n",
      "Success: 28\n",
      "Success: 29\n"
     ]
    }
   ],
   "source": [
    "large_agg_hist_2 = opc.get_agg_hist_value_chunks(server_url, start_time, end_time1, pro_interval1, agg_name, vars_node_ids, 100000, 5000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(large_agg_hist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0\n"
     ]
    }
   ],
   "source": [
    "d_value = opc.get_agg_hist_values_dataframe(server_url, start_time, end_time, pro_interval, agg_name, strings_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjectId</th>\n",
       "      <th>DescendantId</th>\n",
       "      <th>DescendantName</th>\n",
       "      <th>DescendantType</th>\n",
       "      <th>ObjectName</th>\n",
       "      <th>Variable</th>\n",
       "      <th>VariableId</th>\n",
       "      <th>ColPosition</th>\n",
       "      <th>GPSLatitude</th>\n",
       "      <th>GPSLongitude</th>\n",
       "      <th>...</th>\n",
       "      <th>NumberOfModules</th>\n",
       "      <th>NumberOfStrings</th>\n",
       "      <th>RelativeXPosition</th>\n",
       "      <th>RelativeYPosition</th>\n",
       "      <th>RowPosition</th>\n",
       "      <th>SourceTimestamp</th>\n",
       "      <th>Value.Type</th>\n",
       "      <th>Value.Body</th>\n",
       "      <th>StatusCode.Code</th>\n",
       "      <th>StatusCode.Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3:1:SSO.BR-AP</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01</td>\n",
       "      <td>BR-AP-TS11.I01-SM01-CH01</td>\n",
       "      <td>StringSetType</td>\n",
       "      <td>BR-AP</td>\n",
       "      <td>Height</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.03836534366169</td>\n",
       "      <td>-37.7941125402066</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-999.932000000029</td>\n",
       "      <td>1584.95000000112</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-05T07:55:14.544Z</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2157641728</td>\n",
       "      <td>BadNoData</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3:1:SSO.BR-AP</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01</td>\n",
       "      <td>BR-AP-TS11.I01-SM01-CH01</td>\n",
       "      <td>StringSetType</td>\n",
       "      <td>BR-AP</td>\n",
       "      <td>Height</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.03836534366169</td>\n",
       "      <td>-37.7941125402066</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-999.932000000029</td>\n",
       "      <td>1584.95000000112</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-05T08:05:14.544Z</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2157641728</td>\n",
       "      <td>BadNoData</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3:1:SSO.BR-AP</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01</td>\n",
       "      <td>BR-AP-TS11.I01-SM01-CH01</td>\n",
       "      <td>StringSetType</td>\n",
       "      <td>BR-AP</td>\n",
       "      <td>Height</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.03836534366169</td>\n",
       "      <td>-37.7941125402066</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-999.932000000029</td>\n",
       "      <td>1584.95000000112</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-05T08:15:14.544Z</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2157641728</td>\n",
       "      <td>BadNoData</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3:1:SSO.BR-AP</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01</td>\n",
       "      <td>BR-AP-TS11.I01-SM01-CH01</td>\n",
       "      <td>StringSetType</td>\n",
       "      <td>BR-AP</td>\n",
       "      <td>Height</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.03836534366169</td>\n",
       "      <td>-37.7941125402066</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-999.932000000029</td>\n",
       "      <td>1584.95000000112</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-05T08:25:14.544Z</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2157641728</td>\n",
       "      <td>BadNoData</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3:1:SSO.BR-AP</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01</td>\n",
       "      <td>BR-AP-TS11.I01-SM01-CH01</td>\n",
       "      <td>StringSetType</td>\n",
       "      <td>BR-AP</td>\n",
       "      <td>Height</td>\n",
       "      <td>3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.03836534366169</td>\n",
       "      <td>-37.7941125402066</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-999.932000000029</td>\n",
       "      <td>1584.95000000112</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-05T08:35:14.544Z</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2157641728</td>\n",
       "      <td>BadNoData</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ObjectId                                DescendantId  \\\n",
       "0  3:1:SSO.BR-AP  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01   \n",
       "1  3:1:SSO.BR-AP  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01   \n",
       "2  3:1:SSO.BR-AP  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01   \n",
       "3  3:1:SSO.BR-AP  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01   \n",
       "4  3:1:SSO.BR-AP  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01   \n",
       "\n",
       "             DescendantName DescendantType ObjectName Variable  \\\n",
       "0  BR-AP-TS11.I01-SM01-CH01  StringSetType      BR-AP   Height   \n",
       "1  BR-AP-TS11.I01-SM01-CH01  StringSetType      BR-AP   Height   \n",
       "2  BR-AP-TS11.I01-SM01-CH01  StringSetType      BR-AP   Height   \n",
       "3  BR-AP-TS11.I01-SM01-CH01  StringSetType      BR-AP   Height   \n",
       "4  BR-AP-TS11.I01-SM01-CH01  StringSetType      BR-AP   Height   \n",
       "\n",
       "                                          VariableId ColPosition  \\\n",
       "0  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...           1   \n",
       "1  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...           1   \n",
       "2  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...           1   \n",
       "3  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...           1   \n",
       "4  3:1:SSO.BR-AP.AP1.Z1.TS11.I11.01.SM01.CH01.Par...           1   \n",
       "\n",
       "         GPSLatitude       GPSLongitude  ... NumberOfModules NumberOfStrings  \\\n",
       "0  -5.03836534366169  -37.7941125402066  ...               0             2.0   \n",
       "1  -5.03836534366169  -37.7941125402066  ...               0             2.0   \n",
       "2  -5.03836534366169  -37.7941125402066  ...               0             2.0   \n",
       "3  -5.03836534366169  -37.7941125402066  ...               0             2.0   \n",
       "4  -5.03836534366169  -37.7941125402066  ...               0             2.0   \n",
       "\n",
       "   RelativeXPosition RelativeYPosition RowPosition           SourceTimestamp  \\\n",
       "0  -999.932000000029  1584.95000000112           1  2022-07-05T07:55:14.544Z   \n",
       "1  -999.932000000029  1584.95000000112           1  2022-07-05T08:05:14.544Z   \n",
       "2  -999.932000000029  1584.95000000112           1  2022-07-05T08:15:14.544Z   \n",
       "3  -999.932000000029  1584.95000000112           1  2022-07-05T08:25:14.544Z   \n",
       "4  -999.932000000029  1584.95000000112           1  2022-07-05T08:35:14.544Z   \n",
       "\n",
       "  Value.Type Value.Body StatusCode.Code StatusCode.Symbol  \n",
       "0         11          0      2157641728         BadNoData  \n",
       "1         11          0      2157641728         BadNoData  \n",
       "2         11          0      2157641728         BadNoData  \n",
       "3         11          0      2157641728         BadNoData  \n",
       "4         11          0      2157641728         BadNoData  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_value_ids = [opc.create_readvalueids_dict(x,agg_name) for x in vars_node_ids]\n",
    "# Lenght of time series\n",
    "n_datapoints = (pd.to_datetime(end_time) - pd.to_datetime(start_time)).total_seconds()*1000/pro_interval\n",
    "\n",
    "# Number of nodeids to read\n",
    "n_nodeids = len(vars_node_ids)\n",
    "total_datapoints = n_nodeids*n_datapoints\n",
    "\n",
    "chunk_size = 100000\n",
    "# Number of required splits \n",
    "n_time_splits = int(np.ceil(n_datapoints/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_ids(ids_list, n):\n",
    "    \"\"\"Yield successive n-sized chunks from ids_list.\"\"\"\n",
    "    for i in range(0, len(ids_list), n):\n",
    "        yield ids_list[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_batch_size = 1000\n",
    "id_chunk_list = list(chunk_ids(read_value_ids, id_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datetime chunks\n",
    "start_end_list = opc.chunk_datetimes(start_time,end_time, n_time_splits)\n",
    "\n",
    "# Body elements to pass in API request body\n",
    "body_elements = list(itertools.product(start_end_list,id_chunk_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create body chunks\n",
    "body_list = []\n",
    "for x in body_elements:\n",
    "    start_time_new = x[0][0]\n",
    "    end_time_new = x[0][1]\n",
    "    ids = x[1]\n",
    "    body = json.dumps({\n",
    "            \"Connection\": {\n",
    "                \"Url\": server_url,\n",
    "                \"AuthenticationType\": 1\n",
    "            },\n",
    "            \"StartTime\": start_time_new,\n",
    "            \"EndTime\": end_time_new,\n",
    "            \"ProcessingInterval\": pro_interval, \n",
    "            \"ReadValueIds\": ids\n",
    "        \n",
    "        })\n",
    "    body_list.append(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = opc.request_historical_data(body_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded : 0\n",
      "Succeeded : 1\n"
     ]
    }
   ],
   "source": [
    "'''# Request chunkwise data\n",
    "data_list = []\n",
    "for i,body in enumerate(body_list[:2]):\n",
    "    try:\n",
    "        recieved_frame = opc.request_historical_data(body)\n",
    "        print(\"Succeeded : \"+str(i))\n",
    "        data_list+=recieved_frame['HistoryReadResults']\n",
    "    except:\n",
    "        print(\"Failed : \"+str(i))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0\n",
      "Success: 1\n",
      "Success: 2\n",
      "Success: 3\n",
      "Success: 4\n",
      "Success: 5\n",
      "Success: 6\n",
      "Success: 7\n",
      "Success: 8\n",
      "Success: 9\n"
     ]
    }
   ],
   "source": [
    "# Request chunkwise data\n",
    "data_list = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    future_data_map = {executor.submit(opc.request_historical_data, chunk):i for i,chunk in enumerate(body_list[:10])}\n",
    "    for i,future_data in enumerate(concurrent.futures.as_completed(future_data_map)):\n",
    "        try:\n",
    "            print(\"Success: \"+str(i))\n",
    "            recieved_frame = future_data.result()\n",
    "            data_list+=recieved_frame['HistoryReadResults']\n",
    "        except:\n",
    "            print(\"Failed: \"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 0\n",
      "Success: 1\n",
      "Success: 2\n",
      "Success: 3\n",
      "Success: 4\n",
      "Success: 5\n",
      "Success: 6\n",
      "Success: 7\n",
      "Success: 8\n",
      "Success: 9\n"
     ]
    }
   ],
   "source": [
    "# Request chunkwise data\n",
    "data_list = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_data_map = {executor.submit(opc.request_historical_data, chunk):i for i,chunk in enumerate(body_list[:10])}\n",
    "    for i,future_data in enumerate(concurrent.futures.as_completed(future_data_map)):\n",
    "        try:\n",
    "            print(\"Success: \"+str(i))\n",
    "            recieved_frame = future_data.result()\n",
    "            data_list+=recieved_frame['HistoryReadResults']\n",
    "        except:\n",
    "            print(\"Failed: \"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_0)\n",
    "data_0.keys()\n",
    "len(data_0['HistoryReadResults'])\n",
    "data_0['HistoryReadResults'][0]\n",
    "len(data_0['HistoryReadResults'][0]['DataValues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request chunkwise data\n",
    "data_list = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_data_map = {executor.submit(opc.request_historical_data, chunk):i for i,chunk in enumerate(body_list)}\n",
    "    for future_data in concurrent.futures.as_completed(future_data_map):\n",
    "        try:\n",
    "            recieved_frame = future_data.result()\n",
    "        except:\n",
    "            recieved_frame = []\n",
    "        data_list.append(recieved_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# agg_hist_df = opc.get_agg_hist_values(server_url, start_time, end_time, pro_interval, agg_name, vars_node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_data_map = {executor.submit(self.request_historical_data, chunk):i for i,chunk in enumerate(body_list)}\n",
    "            for future_data in concurrent.futures.as_completed(future_data_map):\n",
    "                try:\n",
    "                    recieved_frame = future_data.result()\n",
    "                except:\n",
    "                    recieved_frame = []\n",
    "                data_list.append(recieved_frame)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def get_agg_hist_values_dataframe(self, server_url: str, start_time: str, end_time: str, pro_interval: int, agg_name: str, data_frame : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Make a dataframe of aggregated historical value data\n",
    "\n",
    "        Args:\n",
    "            server_url (str): server connection url\n",
    "            start_time (str): start time of requested data\n",
    "            end_time (str): end time of the requested data\n",
    "            pro_interval (int): interval time of processing in milliseconds\n",
    "            agg_name (str): Name of aggregation\n",
    "            data_frame (pd.DataFrame): Pandas data frame with required columns such as node_ids\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe of the requested historical values\n",
    "        \"\"\"\n",
    "        # JSON normalization of aggregate historical values\n",
    "        df = pd.json_normalize(self.get_agg_hist_values(server_url, start_time, end_time, pro_interval, agg_name, data_frame['VariableId'].to_list())['HistoryReadResults'])\n",
    "        # Concating aggregated historical values to dataframe\n",
    "        data_frame1 = pd.concat([data_frame,df], axis=1).drop(columns=['NodeId.IdType', 'NodeId.Id', 'NodeId.Namespace', 'StatusCode.Code', 'StatusCode.Symbol']) \n",
    "        # Exploding DataValues column\n",
    "        df1 = data_frame1.explode('DataValues').reset_index(drop=True)\n",
    "        # JSON normalization of DataValues\n",
    "        df2 = pd.json_normalize(df1['DataValues'])\n",
    "        # Concatenating dataframes\n",
    "        data_frame2 = pd.concat([df1,df2], axis=1).drop(columns=[\"DataValues\"])\n",
    "        return data_frame2\n",
    "        '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f4da5407c789aed4304d38072e4a31f4393d992bee9448078254d6bd7caaa79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
