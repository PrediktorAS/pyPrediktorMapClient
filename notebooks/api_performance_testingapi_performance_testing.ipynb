{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores both model index and opc ua scripts and contain examples of all the functions to make request to model index api and opc ua api servers.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packeages\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model index functions\n",
    "from pyprediktormapclient.model_index import ModelIndex\n",
    "\n",
    "# Import OPC UA functions\n",
    "from pyprediktormapclient.opc_ua import OPC_UA\n",
    "\n",
    "# Import Analytics Helper\n",
    "from pyprediktormapclient.analytics_helper import AnalyticsHelper\n",
    "\n",
    "# Import \"Dataframer\" Tools\n",
    "from pyprediktormapclient.shared import *\n",
    "\n",
    "# import AUTH_CLIENT\n",
    "from pyprediktormapclient.auth_client import AUTH_CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider obtaining the envrionment variables from .env file if you are running this locally from source.\n",
    "dotenv_path = Path(\".env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "username = os.environ[\"USERNAME\"]\n",
    "password = os.environ[\"PASSWORD\"]\n",
    "opcua_rest_url = os.environ[\"OPC_UA_REST_URL\"]\n",
    "opcua_server_url = os.environ[\"OPC_UA_SERVER_URL\"]\n",
    "model_index_url = os.environ[\"MODEL_INDEX_URL\"]\n",
    "ory_url = os.environ[\"ORY_URL\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ory bearer token\n",
    "auth_client = AUTH_CLIENT(rest_url=ory_url, username=username, password=password)\n",
    "auth_client.request_new_ory_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to ModelIndex APIs \n",
    "model = ModelIndex(url=model_index_url, auth_client=auth_client, session=auth_client.session)\n",
    "\n",
    "# Listed sites on the model index api server\n",
    "namespaces = model.get_namespace_array()\n",
    "# Types of Objects\n",
    "object_types_json = model.get_object_types()\n",
    "object_types = AnalyticsHelper(object_types_json)\n",
    "namespace_list = object_types.namespaces_as_list(namespaces)\n",
    "\n",
    "# Initate the OPC UA API with a fixed namespace list\n",
    "opc_data = OPC_UA(rest_url=opcua_rest_url, opcua_url=opcua_server_url, namespaces=namespace_list, auth_client=auth_client)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from modelindex api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique types of Objects\n",
    "object_types_unique = object_types.dataframe[[\"Id\", \"Name\"]].drop_duplicates()\n",
    "object_types_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the objects of a type\n",
    "sites_json = model.get_objects_of_type(\"SiteType\")\n",
    "\n",
    "# Send the returned JSON into a normalizer to get Id, Type, Name, Props and Vars as columns\n",
    "sites = AnalyticsHelper(sites_json)\n",
    "sites.list_of_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics helper\n",
    "sites.variables_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites.list_of_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the second site\n",
    "first_site_id = sites.list_of_ids()[0]\n",
    "# first_site_id = '14:1:BE.DK-ADU'\n",
    "first_site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stringsets for one park\n",
    "string_sets_for_first_park_as_json = model.get_object_descendants(\n",
    "    \"StringSetType\", [first_site_id], \"PV_Assets\"\n",
    ")\n",
    "string_sets = AnalyticsHelper(string_sets_for_first_park_as_json)\n",
    "string_sets.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 day aggregated historical inverter data in asyncio process\n",
    "one_days_historic_inverter_data2 = await opc_data.get_historical_aggregated_values_async(\n",
    "    start_time=(datetime.datetime.now() - datetime.timedelta(30)),\n",
    "    end_time=(datetime.datetime.now() - datetime.timedelta(29)),\n",
    "    pro_interval=60*1000,\n",
    "    agg_name=\"Average\",\n",
    "    variable_list=string_sets.variables_as_list([\"DCPower\"]),\n",
    "    batch_size=500\n",
    ")\n",
    "one_days_historic_inverter_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stringset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_aggregated_values(opc_data,\n",
    "    start_time, \n",
    "    end_time, \n",
    "    pro_interval, \n",
    "    agg_name, \n",
    "    variable_list\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "    extended_variables = [{\"NodeId\": var, \"AggregateName\": agg_name} for var in vars]\n",
    "\n",
    "    body = {\n",
    "        **opc_data.body, \n",
    "        \"StartTime\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "        \"EndTime\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "        \"ProcessingInterval\": pro_interval, \n",
    "        \"AggregateName\": agg_name,\n",
    "        \"ReadValueIds\": extended_variables\n",
    "    }\n",
    "    print(body)\n",
    "\n",
    "    content = request_from_api(\n",
    "        rest_url=opcua_rest_url, \n",
    "        method=\"POST\", \n",
    "        endpoint=\"values/historicalaggregated\", \n",
    "        data=json.dumps(body, default=opc_data.json_serial), \n",
    "        headers=opc_data.headers, \n",
    "        extended_timeout=True\n",
    "    )\n",
    "    print(content)\n",
    "    df_result = pd.json_normalize(\n",
    "        content, \n",
    "        record_path=['HistoryReadResults', 'DataValues'], \n",
    "        meta=[['HistoryReadResults', 'NodeId', 'IdType'], ['HistoryReadResults', 'NodeId','Id'],['HistoryReadResults', 'NodeId','Namespace']\n",
    "        ]\n",
    "    )\n",
    "    columns = {\n",
    "        \"Value.Type\": \"ValueType\",\n",
    "        \"Value.Body\": \"Value\",\n",
    "        \"StatusCode.Symbol\": \"StatusSymbol\",\n",
    "        \"StatusCode.Code\": \"StatusCode\",\n",
    "        \"SourceTimestamp\": \"Timestamp\",\n",
    "        \"HistoryReadResults.NodeId.IdType\": \"IdType\",\n",
    "        \"HistoryReadResults.NodeId.Id\": \"Id\",\n",
    "        \"HistoryReadResults.NodeId.Namespace\": \"Namespace\",\n",
    "    }\n",
    "    return opc_data._process_df(df_result, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=(datetime.datetime.now() - datetime.timedelta(30))\n",
    "end_time=(datetime.datetime.now() - datetime.timedelta(29))\n",
    "pro_interval=600000\n",
    "agg_name=\"Average\"\n",
    "variable_list=string_sets.variables_as_list([\"DCPower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_aggregated_values(opc_data,\n",
    "                                     start_time, \n",
    "                                     end_time, \n",
    "                                     pro_interval, \n",
    "                                     agg_name, \n",
    "                                     variable_list) -> pd.DataFrame:\n",
    "    vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "    batch_size = 100\n",
    "    batches = [vars[i:i + batch_size] for i in range(0, len(vars), batch_size)]\n",
    "    \n",
    "    combined_df = pd.DataFrame()  \n",
    "    \n",
    "    for batch in batches:\n",
    "        extended_variables = [{\"NodeId\": var, \"AggregateName\": agg_name} for var in batch]\n",
    "        \n",
    "        body = {\n",
    "            **opc_data.body, \n",
    "            \"StartTime\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "            \"EndTime\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "            \"ProcessingInterval\": pro_interval, \n",
    "            \"AggregateName\": agg_name,\n",
    "            \"ReadValueIds\": extended_variables\n",
    "        }\n",
    "        \n",
    "        content = request_from_api(\n",
    "            rest_url=opcua_rest_url, \n",
    "            method=\"POST\", \n",
    "            endpoint=\"values/historicalaggregated\", \n",
    "            data=json.dumps(body, default=opc_data.json_serial), \n",
    "            headers=opc_data.headers, \n",
    "            extended_timeout=True\n",
    "        )\n",
    "        \n",
    "        df_result = pd.json_normalize(\n",
    "            content, \n",
    "            record_path=['HistoryReadResults', 'DataValues'], \n",
    "            meta=[['HistoryReadResults', 'NodeId', 'IdType'], ['HistoryReadResults', 'NodeId','Id'],['HistoryReadResults', 'NodeId','Namespace']]\n",
    "        )\n",
    "        \n",
    "        if combined_df.empty:\n",
    "            combined_df = df_result\n",
    "        else:\n",
    "            combined_df = pd.concat([combined_df, df_result], ignore_index=True)\n",
    "    \n",
    "    columns = {\n",
    "        \"Value.Type\": \"ValueType\",\n",
    "        \"Value.Body\": \"Value\",\n",
    "        \"StatusCode.Symbol\": \"StatusSymbol\",\n",
    "        \"StatusCode.Code\": \"StatusCode\",\n",
    "        \"SourceTimestamp\": \"Timestamp\",\n",
    "        \"HistoryReadResults.NodeId.IdType\": \"IdType\",\n",
    "        \"HistoryReadResults.NodeId.Id\": \"Id\",\n",
    "        \"HistoryReadResults.NodeId.Namespace\": \"Namespace\",\n",
    "    }\n",
    "    \n",
    "    return opc_data._process_df(combined_df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_aggregated_values(opc_data,\n",
    "                                     start_time, \n",
    "                                     end_time, \n",
    "                                     pro_interval, \n",
    "                                     agg_name, \n",
    "                                     variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_aggregated_values(opc_data, start_time, end_time, pro_interval, agg_name, variable_list) -> pd.DataFrame:\n",
    "    vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "    batch_size = 150\n",
    "    batches = [vars[i:i + batch_size] for i in range(0, len(vars), batch_size)]\n",
    "\n",
    "    def process_batch(batch):\n",
    "        extended_variables = [{\"NodeId\": var, \"AggregateName\": agg_name} for var in batch]\n",
    "        body = {\n",
    "            **opc_data.body,\n",
    "            \"StartTime\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"EndTime\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"ProcessingInterval\": pro_interval,\n",
    "            \"AggregateName\": agg_name,\n",
    "            \"ReadValueIds\": extended_variables\n",
    "        }\n",
    "        content = request_from_api(\n",
    "            rest_url=opcua_rest_url,\n",
    "            method=\"POST\",\n",
    "            endpoint=\"values/historicalaggregated\",\n",
    "            data=json.dumps(body, default=opc_data.json_serial),\n",
    "            headers=opc_data.headers,\n",
    "            extended_timeout=True\n",
    "        )\n",
    "        return pd.json_normalize(\n",
    "            content,\n",
    "            record_path=['HistoryReadResults', 'DataValues'],\n",
    "            meta=[['HistoryReadResults', 'NodeId', 'IdType'], ['HistoryReadResults', 'NodeId', 'Id'], ['HistoryReadResults', 'NodeId', 'Namespace']]\n",
    "        )\n",
    "\n",
    "    dataframes = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_batch = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            dataframes.append(future.result())\n",
    "\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True) if dataframes else pd.DataFrame()\n",
    "\n",
    "    columns = {\n",
    "        \"Value.Type\": \"ValueType\",\n",
    "        \"Value.Body\": \"Value\",\n",
    "        \"StatusCode.Symbol\": \"StatusSymbol\",\n",
    "        \"StatusCode.Code\": \"StatusCode\",\n",
    "        \"SourceTimestamp\": \"Timestamp\",\n",
    "        \"HistoryReadResults.NodeId.IdType\": \"IdType\",\n",
    "        \"HistoryReadResults.NodeId.Id\": \"Id\",\n",
    "        \"HistoryReadResults.NodeId.Namespace\": \"Namespace\",\n",
    "    }\n",
    "\n",
    "    return opc_data._process_df(combined_df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "extended_variables = [{\"NodeId\": var, \"AggregateName\": agg_name} for var in vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    **opc_data.body,\n",
    "    \"StartTime\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"EndTime\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"ProcessingInterval\": pro_interval,\n",
    "    \"AggregateName\": agg_name,\n",
    "    \"ReadValueIds\": extended_variables\n",
    "}\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_historical_aggregated_values(opc_data,\n",
    "                                     start_time, \n",
    "                                     end_time, \n",
    "                                     pro_interval, \n",
    "                                     agg_name, \n",
    "                                     variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now() - relativedelta(months=1)\n",
    "end_time = datetime.now()\n",
    "get_historical_aggregated_values(opc_data,\n",
    "                                     start_time, \n",
    "                                     end_time, \n",
    "                                     pro_interval, \n",
    "                                     agg_name, \n",
    "                                     variable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History data for 1 day, 10 min aggregate - stringsets\n",
    "history_agg = opc_data.get_historical_aggregated_values(\n",
    "    start_time=(datetime.datetime.now() - datetime.timedelta(30)),\n",
    "    end_time=(datetime.datetime.now() - datetime.timedelta(29)),\n",
    "    pro_interval=600000,\n",
    "    agg_name=\"Average\",\n",
    "    variable_list=inverters.variables_as_list([\"DCPower\"]),\n",
    ")\n",
    "history_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from pydantic import BaseModel, AnyUrl\n",
    "from datetime import timedelta\n",
    "import asyncio\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variables(BaseModel):\n",
    "    \"\"\"Helper class to parse all values api's.\n",
    "    Variables are described in https://reference.opcfoundation.org/v104/Core/docs/Part3/8.2.1/\n",
    "\n",
    "        Variables:\n",
    "            Id: str - Id of the signal, e.g. SSO.EG-AS.WeatherSymbol\n",
    "            Namespace: int - Namespace on the signal, e.g. 2.\n",
    "            IdType: int - IdTypes described in https://reference.opcfoundation.org/v104/Core/docs/Part3/8.2.3/.\n",
    "    \"\"\"\n",
    "    Id: str\n",
    "    Namespace: int\n",
    "    IdType: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_async_api_request(opc_data, start_time: datetime, end_time: datetime, pro_interval: int, agg_name: str, variable_list: list[Variables]) -> dict:\n",
    "        \"\"\"Make API request for the given time range and variable list\"\"\"\n",
    "\n",
    "        # Creating a new variable list to remove pydantic models\n",
    "        vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "\n",
    "        extended_variables = [\n",
    "            {\n",
    "                    \"NodeId\": var,\n",
    "                    \"AggregateName\": agg_name,\n",
    "            }\n",
    "            for var in vars\n",
    "        ]\n",
    "        body = copy.deepcopy(opc_data.body)\n",
    "        body[\"StartTime\"] = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        body[\"EndTime\"] = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        body[\"ProcessingInterval\"] = pro_interval\n",
    "        body[\"ReadValueIds\"] = extended_variables\n",
    "        body[\"AggregateName\"] = agg_name\n",
    "\n",
    "            # Make API request using aiohttp session\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(\n",
    "                f\"{opcua_rest_url}values/historicalaggregated\",\n",
    "                data=json.dumps(body, default=opc_data.json_serial),\n",
    "                headers=opc_data.headers,\n",
    "                timeout=aiohttp.ClientTimeout(total=None)  \n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                content = await response.json()\n",
    "\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "vars1 = vars[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_variables = [\n",
    "            {\n",
    "                    \"NodeId\": var,\n",
    "                    \"AggregateName\": agg_name,\n",
    "            }\n",
    "            for var in vars1\n",
    "]\n",
    "len(extended_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = copy.deepcopy(opc_data.body)\n",
    "body[\"StartTime\"] = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "body[\"EndTime\"] = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "body[\"ProcessingInterval\"] = pro_interval\n",
    "body[\"ReadValueIds\"] = extended_variables\n",
    "body[\"AggregateName\"] = agg_name\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{opcua_rest_url}values/historicalaggregated\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=json.dumps(body, default=opc_data.json_serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_value_ids = data_dict['ReadValueIds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(read_value_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=opc_data.headers\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout=aiohttp.ClientTimeout(total=None) \n",
    "timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.post(\n",
    "        f\"{opcua_rest_url}values/historicalaggregated\",\n",
    "        data=json.dumps(body, default=opc_data.json_serial),\n",
    "        headers=opc_data.headers,\n",
    "        timeout=aiohttp.ClientTimeout(total=None)  \n",
    "    ) as response:\n",
    "        response.raise_for_status()\n",
    "        content = await response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_batches(start_time: datetime, end_time: datetime, pro_interval: int, batch_size: int) -> list[tuple]:\n",
    "    \"\"\"Generate time batches based on start time, end time, processing interval, and batch size\"\"\"\n",
    "\n",
    "    total_time_range = end_time - start_time\n",
    "    pro_interval_seconds = (pro_interval / 1000)\n",
    "    total_data_points = (total_time_range.total_seconds() // pro_interval_seconds) + 1\n",
    "\n",
    "    total_batches = math.ceil(total_data_points / batch_size)\n",
    "    actual_batch_size = math.ceil(total_data_points / total_batches)\n",
    "\n",
    "    time_batches = [\n",
    "        (start_time + timedelta(seconds=(i * actual_batch_size * pro_interval_seconds)),\n",
    "        start_time + timedelta(seconds=((i + 1) * actual_batch_size * pro_interval_seconds)) - timedelta(seconds=pro_interval_seconds))\n",
    "        for i in range(total_batches)\n",
    "    ]\n",
    "\n",
    "    return time_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variable_batches(variable_list: list[Variables], batch_size: int) -> list[list[Variables]]:\n",
    "    \"\"\"Generate variable batches based on the variable list and batch size\"\"\"\n",
    "\n",
    "    variable_batches = [\n",
    "        variable_list[i:i + batch_size] for i in range(0, len(variable_list), batch_size)\n",
    "    ]\n",
    "\n",
    "    return variable_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_api_response(opc_data, response: dict) -> pd.DataFrame:\n",
    "        \"\"\"Process the API response and return the result dataframe\"\"\"\n",
    "        \n",
    "        df_result = pd.json_normalize(response, record_path=['HistoryReadResults', 'DataValues'], \n",
    "                                      meta=[['HistoryReadResults', 'NodeId', 'IdType'], ['HistoryReadResults', 'NodeId','Id'],\n",
    "                                            ['HistoryReadResults', 'NodeId','Namespace']] )\n",
    "\n",
    "        for i, row in df_result.iterrows():\n",
    "            if not math.isnan(row[\"Value.Type\"]):\n",
    "                value_type = opc_data._get_value_type(int(row[\"Value.Type\"])).get(\"type\")\n",
    "                df_result.at[i, \"Value.Type\"] = str(value_type)\n",
    "\n",
    "        df_result.rename(\n",
    "            columns={\n",
    "                \"Value.Type\": \"ValueType\",\n",
    "                \"Value.Body\": \"Value\",\n",
    "                \"StatusCode.Symbol\": \"StatusSymbol\",\n",
    "                \"StatusCode.Code\": \"StatusCode\",\n",
    "                \"SourceTimestamp\": \"Timestamp\",\n",
    "                \"HistoryReadResults.NodeId.IdType\": \"Id\",\n",
    "                \"HistoryReadResults.NodeId.Namespace\": \"Namespace\",\n",
    "            },\n",
    "            errors=\"raise\",\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_historical_aggregated_values_async(\n",
    "    opc_data,\n",
    "    start_time: datetime,\n",
    "    end_time: datetime,\n",
    "    pro_interval: int,\n",
    "    agg_name: str,\n",
    "    variable_list: list[Variables],\n",
    "    batch_size: int = 1000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Request historical aggregated values from the OPC UA server with batching\"\"\"\n",
    "\n",
    "    \n",
    "    time_batches = generate_time_batches(start_time, end_time, pro_interval, batch_size)\n",
    "    variable_batches = generate_variable_batches(variable_list, batch_size)\n",
    "\n",
    "    # Creating tasks for each API request and gathering the results\n",
    "    tasks = []\n",
    "\n",
    "    for time_batch_start, time_batch_end in time_batches:\n",
    "        for variable_sublist in variable_batches:\n",
    "            task = asyncio.create_task(\n",
    "                make_async_api_request(opc_data, time_batch_start, time_batch_end, pro_interval, agg_name, variable_sublist)\n",
    "            ) \n",
    "            tasks.append(task)\n",
    "    \n",
    "    # Execute all tasks concurrently and gather their results\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Processing the API responses\n",
    "    result_list = []\n",
    "    for idx, batch_response in enumerate(responses):\n",
    "        \n",
    "        batch_result = process_api_response(opc_data, batch_response)\n",
    "        result_list.append(batch_result)\n",
    "       \n",
    "    result_df = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 day aggregated historical inverter data in asyncio process\n",
    "one_days_historic_inverter_data2 = await get_historical_aggregated_values_async(\n",
    "    opc_data,\n",
    "    start_time=(datetime.datetime.now() - datetime.timedelta(30)),\n",
    "    end_time=(datetime.datetime.now() - datetime.timedelta(29)),\n",
    "    pro_interval=60*1000,\n",
    "    agg_name=\"Average\",\n",
    "    variable_list=string_sets.variables_as_list([\"DCPower\"]),\n",
    "    batch_size=100\n",
    ")\n",
    "one_days_historic_inverter_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_time_chunks(start_time: datetime, end_time: datetime):\n",
    "    \"\"\"Generate time chunks between start_time and end_time, each chunk_duration_minutes long.\"\"\"\n",
    "    delta = timedelta(minutes=60)\n",
    "    current_time = start_time\n",
    "    while current_time < end_time:\n",
    "        chunk_end_time = min(current_time + delta, end_time)\n",
    "        yield (current_time, chunk_end_time)\n",
    "        current_time = chunk_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_async_api_request(opc_data, start_time: datetime, end_time: datetime, pro_interval: int, agg_name: str, variable_list: list[Variables], max_data_points=500) -> dict:\n",
    "    \"\"\"Make API request for the given time range and variable list, with additional chunking based on data points.\"\"\"\n",
    "\n",
    "    def chunk_list(lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    async def fetch_data_for_time_period(session, vars_chunk, start, end):\n",
    "        \"\"\"Fetch data for a given time period and chunk of variables.\"\"\"\n",
    "        extended_variables = [{\"NodeId\": var, \"AggregateName\": agg_name} for var in vars_chunk]\n",
    "        body = copy.deepcopy(opc_data.body)\n",
    "        body[\"StartTime\"] = start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        body[\"EndTime\"] = end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        body[\"ProcessingInterval\"] = pro_interval\n",
    "        body[\"ReadValueIds\"] = extended_variables\n",
    "        body[\"AggregateName\"] = agg_name\n",
    "\n",
    "        async with session.post(\n",
    "            f\"{opcua_rest_url}values/historicalaggregated\",\n",
    "            data=json.dumps(body, default=str),\n",
    "            headers=opc_data.headers,\n",
    "            timeout=aiohttp.ClientTimeout(total=None)\n",
    "        ) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.json()\n",
    "\n",
    "    # Creating a new variable list to remove pydantic models\n",
    "    vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "    chunk_size = 5  # Chunk size for node IDs\n",
    "    vars_chunks = list(chunk_list(vars, chunk_size))\n",
    "\n",
    "    all_responses = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for vars_chunk in vars_chunks:\n",
    "            # Generate time chunks for the given time period\n",
    "            async for start, end in generate_time_chunks(start_time, end_time):\n",
    "                content = await fetch_data_for_time_period(session, vars_chunk, start, end)\n",
    "                all_responses.append(content)\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_async_api_request(opc_data, start_time: datetime, end_time: datetime, pro_interval: int, agg_name: str, variable_list: list[Variables]) -> dict:\n",
    "    \"\"\"Make API request for the given time range and variable list\"\"\"\n",
    "\n",
    "    def chunk_list(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    # Creating a new variable list to remove pydantic models\n",
    "    vars = opc_data._get_variable_list_as_list(variable_list)\n",
    "\n",
    "    chunk_size = 150  \n",
    "    vars_chunks = list(chunk_list(vars, chunk_size))\n",
    "\n",
    "    all_responses = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for vars_chunk in vars_chunks:\n",
    "            extended_variables = [{\"NodeId\": var, \"AggregateName\": agg_name} for var in vars_chunk]\n",
    "            body = copy.deepcopy(opc_data.body)\n",
    "            body[\"StartTime\"] = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            body[\"EndTime\"] = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            body[\"ProcessingInterval\"] = pro_interval\n",
    "            body[\"ReadValueIds\"] = extended_variables\n",
    "            body[\"AggregateName\"] = agg_name\n",
    "\n",
    "            async with session.post(\n",
    "                f\"{opcua_rest_url}values/historicalaggregated\",\n",
    "                data=json.dumps(body, default=str),\n",
    "                headers=opc_data.headers,\n",
    "                timeout=aiohttp.ClientTimeout(total=None)\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                content = await response.json()\n",
    "                all_responses.append(content)  \n",
    "\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_chunks(start_time: datetime, end_time: datetime, interval_hours: int) -> List[Tuple[datetime, datetime]]:\n",
    "    \"\"\"Generate time chunks within the given start and end time with specified interval in hours.\"\"\"\n",
    "    delta = timedelta(hours=interval_hours)\n",
    "    current_time = start_time\n",
    "    chunks = []\n",
    "\n",
    "    while current_time < end_time:\n",
    "        chunk_end_time = min(current_time + delta, end_time)  \n",
    "        chunks.append((current_time, chunk_end_time))\n",
    "        current_time += delta\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 day aggregated historical inverter data in asyncio process\n",
    "one_days_historic_inverter_data2 = await make_async_api_request(\n",
    "    opc_data,\n",
    "    start_time=(datetime.datetime.now() - datetime.timedelta(30)),\n",
    "    end_time=(datetime.datetime.now() - datetime.timedelta(29)),\n",
    "    pro_interval=60*1000,\n",
    "    agg_name=\"Average\",\n",
    "    variable_list=string_sets.variables_as_list([\"DCPower\"])\n",
    ")\n",
    "one_days_historic_inverter_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b866f0bc560289bf4bb2415ae9074243764eb008c10d00a1da29433677418de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
